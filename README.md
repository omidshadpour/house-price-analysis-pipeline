# ğŸ  House Price Analysis & Prediction Pipeline  
A complete end-to-end Machine Learning pipeline for predicting house prices using Python, Scikit-Learn, and XGBoost.  
This project demonstrates a clean, modular, and production-ready ML workflow â€” ideal for real-world applications and portfolio use.

---

## ğŸ“Œ Project Overview
This project builds a full ML pipeline to predict house prices based on the Ames Housing dataset.  
It includes:

- Data preprocessing  
- Feature engineering  
- Model training & hyperparameter tuning  
- Model comparison  
- Feature importance visualization  
- Saving the best model for deployment  

The entire workflow is modular, scalable, and follows industry-standard practices.

---

## ğŸ“ Project Structure

house_price_project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pkl
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/
â”‚       â””â”€â”€ feature_importance_top20.png
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ train_models.py
â”‚   â””â”€â”€ feature_importance.py
â”‚
â”œâ”€â”€ main.py
â””â”€â”€ README.md

---

## âš™ï¸ Pipeline Steps

### **1. Data Preprocessing**
Handled by `DataPreprocessor`:
- Load raw data  
- Drop ID columns  
- Remove high-missing features  
- Remove low-variance features  
- Fill missing values (median/mode)  

### **2. Feature Engineering**
Handled by `FeatureEngineer`:
- Label Encoding for categorical features  
- Standard Scaling for numerical features  

### **3. Model Training**
Handled by `ModelTrainer`:
- Linear Regression  
- Random Forest (with GridSearchCV)  
- XGBoost (with GridSearchCV)  
- Evaluation using RMSE & RÂ²  
- Automatic best model selection  

### **4. Feature Importance**
Handled by `FeatureImportancePlotter`:
- Extract feature importances  
- Plot top 20 features  
- Save visualization  

---

## ğŸ§  Best Model Results

| Model             | RMSE        | RÂ²       |
|------------------|-------------|----------|
| LinearRegression | 34281.66    | 0.8468   |
| RandomForest     | 28274.78    | 0.8957   |
| **XGBoost**      | **26917.13** | **0.9055** |

**XGBoost** achieved the best performance and is saved as the final model.

---

## ğŸ” Feature Importance (Top 20)

The project generates a bar plot showing the most influential features.  
Example output:

OverallQual
FullBath
GarageCars
BsmtQual
GrLivArea
...

The plot is saved at:

reports/figures/feature_importance_top20.png

---

## â–¶ï¸ How to Run the Pipeline

### **1. Install dependencies**
pip install -r requirements.txt


### **2. Run the main pipeline**
python main.py


This will:

- Preprocess data  
- Engineer features  
- Train all models  
- Compare performance  
- Save the best model  
- Generate feature importance plot  

---

## ğŸ§© Technologies Used
- Python  
- Pandas  
- NumPy  
- Scikit-Learn  
- XGBoost  
- Seaborn / Matplotlib  
- Joblib  

---

## ğŸ“Œ Key Highlights
- Fully modular ML architecture  
- Clean and scalable codebase  
- Automated model comparison  
- Production-ready preprocessing pipeline  
- Professional project structure  
- Ideal for ML portfolios and freelance work  

---

## ğŸ“¬ Contact
If you have questions or want to collaborate, feel free to reach out.

Author: Omid Shadpour
Project Type: Portfolio / ML Pipeline


---

â­ If you like this project, consider giving it a star on GitHub!
